# -*- coding: utf-8 -*-
"""Internship_Assignment.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1odUFw-S3ZkBcPvtiqQB5f9Wh_S-xUP9j

#Import Libraries
"""

import tensorflow as tf
import keras
import numpy as np
import pandas as pd
from sklearn import preprocessing
from sklearn import model_selection
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix,accuracy_score

"""#Importing Dataset For the task completion"""

dataset =pd.read_csv("musk.csv")

dataset.shape

"""Removing the unsuited features from the dataset"""

del dataset['conformation_name']
del dataset['ID']

"""Checking for the Null Values and removing if any"""

dataset.isnull().sum()

"""Overview of the dataset"""

dataset.head(4)

"""Label and Features splitting from the dataset"""

label =dataset.iloc[:,-1:].values
data =dataset.iloc[:,:-1].values

"""##Preprocessing the dataset

* Applying LabelEncoding on the dataset
* Applying L2 normalization on dataset
* Train Test split in ratio of 80/20
"""

data[:,0:1] =preprocessing.OrdinalEncoder().fit_transform(data[:,0:1])

data[:,1:] = preprocessing.normalize(data[:,1:], norm='l2')

train_X, test_X, train_y, test_y =model_selection.train_test_split(data, label, train_size=0.8)

"""from sklearn.preprocessing import StandardScaler
sc_x = StandardScaler()
train_X = sc_x.fit_transform(train_X)
test_X = sc_x.transform(test_X)

#Defining the Model
"""

classifier =tf.keras.models.Sequential([
    tf.keras.layers.Dense(units=256, activation='relu', input_dim=167),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(512, activation="relu"),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(512, activation="relu"),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(128, activation="relu"),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(64, activation="relu"),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(1,activation="sigmoid")
])

classifier.summary()

classifier.compile(optimizer ='adam', loss = 'binary_crossentropy', metrics = ['accuracy'])

history =classifier.fit(train_X, train_y, batch_size = 128, epochs = 50, verbose = 1, validation_data = (test_X, test_y))

"""#Visualising of the Training and Validation loss as well as accuracy"""

# sets for each training epoch
acc      = history.history[     'acc' ]
val_acc  = history.history[ 'val_acc' ]
loss     = history.history[    'loss' ]
val_loss = history.history['val_loss' ]

epochs   = range(len(acc)) # Get number of epochs

# Plot training and validation accuracy per epoch

plt.plot  ( epochs,     acc , color='red'  ,label="training")
plt.plot  ( epochs, val_acc , color='blue' ,label="validation")
plt.title ('Training and validation accuracy')
plt.legend()
plt.figure()

# Plot training and validation loss per epoch

plt.plot  ( epochs,     loss ,color='red'  ,label="training")
plt.plot  ( epochs, val_loss ,color='blue' ,label="validation")
plt.legend()
plt.title ('Training and validation loss'   )

"""#Prediction results on the Validation Split and Analysis
* Accuracy 
* Precision
* Recall
* F1
"""

result =classifier.predict(test_X)
result =(result>0.5)

cm =confusion_matrix(test_y ,result)
cm

accuracy =accuracy_score(test_y,result)
precision = cm[1][1]/(cm[1][1] + cm[0][1])
Recall = cm[1][1]/(cm[1][1] + cm[1][0])
f1 = (2*precision*Recall)/(precision+Recall)

print("Accuracy of model",accuracy)
print("Precision of model",precision)
print("Recall of model",Recall)
print("F1 score of model",f1)

"""#saving of model bias"""

classifier.save_weights('classifier_weights.h5')

